---
title: "Student Performance Predictions"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
student_por = read.csv('student+performance/student-por.csv', sep = ';',  stringsAsFactors = TRUE)
student_math = read.csv('student+performance/student-mat.csv', sep = ';', stringsAsFactors = TRUE)
all_data = rbind(student_por, student_math)
summary(all_data)
```

```{r}
head(all_data)
write.csv(all_data, 'student+performance/merged_data.csv', row.names = FALSE)
```

Covariance
```{r}
library(corrplot)
numAll = all_data[ ,sapply(all_data, is.numeric)]
corrMat = cor(numAll)
corrplot(corrMat, type = "upper", number.cex = 1.5, tl.cex = 1.5)
```

```{r}
par(mfrow = c(2, 3))
namesInData = names(all_data)
for (i in namesInData){
  if (i != "G3") {
    boxplot(all_data$G3 ~ all_data[, i], xlab = i, ylab = "Final Grade")
  }
  
}
```
- We see that study time gives a higher final grade along with G1 and G2, which are first and second period grades for the class. We notice the the number of failures the student has had gives and impact for the final grade of there current class. 
```{r}
par(mfrow = c(2, 3))
namesInData = names(all_data)
for (i in namesInData){
  if (i != "G3") {
      plot(all_data$G3 ~ all_data[, i], xlab = i, ylab = "Final Grade")
  }
  
}
```
- We need to convert some of the data to factor for example study and travel time since it just classifying a range. In studytime 1 is for less then 2 hours, 2 is for 2 to 5 hours, 3 is for 5 to 10 hours, and 4 is for 10 hours or more. We would have to change these variables so it doesn't mess with our models.
```{r}
nameOfNumericCols = names(all_data)[sapply(all_data, is.numeric)]
nameOfNumericCols

```

- These are the values that are considered numeric, but we need to change the ones that are just numeric values to define a category. We could change the values for health, Walc, Dalc, goout, and freetime to factors since it just stating a category.
```{r}

```

```{r}
set.seed(1)
n = nrow(all_data)
nFeatures= ncol(all_data) - 1
percentTrain = .7
nTrain = n*percentTrain
trainIdx = sample(1:n, nTrain)
trainData = all_data[trainIdx, ]
allLinearFit = lm(G3 ~ ., trainData)
testData = all_data[-trainIdx,]
summary(allLinearFit)
preds = predict(allLinearFit, testData)
sqrt(mean((testData$G3 - preds)^2))
```

```{r}

linearFit = lm(G3 ~ G1 + G2 + studytime + failures, trainData)
summary(linearFit)

preds = predict(linearFit, testData)
sqrt(mean((testData$G3 - preds)^2))
```
-best Subset selection
```{r}
#install.packages("leaps")
library(leaps)
regfit.full = regsubsets(G3 ~ ., trainData, nvmax = nFeatures)
regfit.summary = summary(regfit.full)
regfit.summary
```


```{r}

regfit.adjr2 = regfit.summary$adjr2
plot(regfit.summary$rsq)
plot(regfit.summary$rss)
plot(regfit.summary$cp)
plot(regfit.summary$bic)
plot(regfit.adjr2, type = 'b', ylab = "Adjusted R^2")
which.max(regfit.adjr2)
which.min(regfit.summary$cp)
which.min(regfit.summary$bic)
regfit.adjr2[14]
coef(regfit.full, id=14)
coef(regfit.full, id=10)
coef(regfit.full, id=2)
```


- The best model according to the training data is with 14 variables. The variables are address, Pstatus, Fjob, traveltime, failures, famsup, paid, activities, absences, G1, and G2. This is not a total of 14 because it has dummy variables within Fjob, and famsup. We can try to make a model with these variables. This turns out to be worse then forward selection so we should try to find the model that has the best CV.

```{r}
bestSub14fit = lm(G3 ~ address + Pstatus + Fjob + traveltime + failures + famsup + paid + activities + absences + G1 + G2, trainData)
summary(bestSub14fit)

preds = predict(bestSub14fit, testData)
sqrt(mean((testData$G3 - preds)^2))
```

```{r}
bestSub10fit = lm(G3 ~ Pstatus + Fjob + traveltime + failures + paid + famsup + absences + G1 + G2, trainData)
summary(bestSub10fit)

preds = predict(bestSub10fit, testData)
sqrt(mean((testData$G3 - preds)^2))
```

- Using validation set for best subset selection
```{r}
set.seed(1)
percentVal = .2
nValidation = percentVal*n
valD = sample(1:nTrain, nValidation)
subSetVal = trainData[valD,] 
subSetTrain = trainData[-valD,]
```

```{r}
regfit.best.validate = regsubsets(G3 ~ ., subSetTrain, nvmax = nFeatures)
```

- the model with the lowest validation error is one that uses 7 variables. They are traveltime, failures, famsup, paid, absences, G1, and G2. Now we should train a model using the full training set and check the testing error.
```{r}
val.mat = model.matrix(G3 ~ ., subSetVal)
val.errors = rep(NA, nFeatures)
for (i in 1:(ncol(trainData) - 1)) {
  coefi = coef(regfit.best.validate, id = i)
  pred = val.mat[ , names(coefi)] %*% coefi
  val.errors[i] = mean((subSetVal$G3 - pred)^2)
}
which.min(val.errors)
coef(regfit.best.validate, which.min(val.errors))
```
```{r}
coef(regfit.full, 5)
```

-- This has the lowest RMSE 
```{r}
bestSub5Fit = lm(G3 ~ failures + paid + absences + G1 + G2, trainData)
summary(bestSub5Fit)
pred = predict(bestSub5Fit, testData)
plot(pred, testData$G3, xlab = "Predicted G3", ylab = "Actual G3", main = "Predicted vs Actual Final Grades (Best 5  Subset Validation Set)")
sqrt(mean((testData$G3 - pred)^2))
```
- Try doing CV with best subset. Must make prediction function. According to the output we should use 2 features for the lowest mean cv error
```{r}
predict.regsubsets = function(object, newData, id, ...) {
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newData)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[ , xvars] %*% coefi
}
# 10 fold CV
k = 10
set.seed(1)
folds = sample(rep(1:k, length = nTrain))
cv.errors = matrix(NA, k, nFeatures, dimnames = list(NULL, paste(1:nFeatures)))

for (j in 1:k) {
  best.fit = regsubsets(G3 ~ ., trainData[folds != j, ], nvmax = nFeatures)
  for (i in 1:nFeatures) {
    pred = predict(best.fit, trainData[folds == j, ], nvmax = nFeatures, id = i)
    cv.errors[j, i] = mean((trainData$G3[folds == j] - pred)^2) 
  }
}

mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type= "b", ylab = "CV Error", main = "CV Error vs number of best subset Features")
```
```{r}
which.min(mean.cv.errors)
coef(regfit.full, which.min(mean.cv.errors))
```
```{r}
bestSub2fit = lm(G3 ~ failures + G2, trainData)
summary(bestSub2fit)
pred = predict(bestSub2fit, testData)
sqrt(mean((testData$G3 - pred)^2))
```

```{r}
coef(regfit.full, 9)

bestSub9fit <- lm(G3 ~ Pstatus + Fjob + traveltime + failures + famsup + paid + absences + G1 + G2, data = trainData)
summary(bestSub9fit)
pred = predict(bestSub9fit, newdata = testData)
sqrt(mean((testData$G3 - pred)^2))
```

- Backward Selection
```{r}
fullFit = lm(G3 ~ ., trainData)
backward_model = step(fullFit, direction = "backward", trace = F)
summary(backward_model)
preds = predict(backward_model, testData)
sqrt(mean((testData$G3 - preds)^2))
```

- Forward Selection
```{r}
nullFit = lm(G3 ~ 1, trainData)
fullFit = lm(G3 ~ ., trainData)

forward_model = step(nullFit, scope = list(lower = nullFit, upper = fullFit), direction = "forward", trace = F)
summary(forward_model)
preds = predict(forward_model, testData)
sqrt(mean((testData$G3 - preds)^2))
```
- The forward and backward selection give the same output so it would be the same rmse.

```{r}
plot(preds, testData$G3, xlab = "Predicted G3", ylab = "Actual G3", main = "Predicted vs Actual Final Grades")
```

- Noticed that the number of absences increases the predicted final grade, which should not be the case, so we should a model without the absences.
```{r}
forward_model_noAbsence = lm(formula = G3 ~ G2 + failures + G1 + paid + famsup + 
    traveltime + Pstatus, data = trainData)
summary(forward_model_noAbsence)
preds = predict(forward_model_noAbsence, testData)
sqrt(mean((testData$G3 - preds)^2))
```

- Try ridge regression
```{r}
#install.packages("glmnet")
library(glmnet)
x = model.matrix(G3 ~ ., all_data)[, -1]
y = all_data$G3
grid = 10^seq(10, -2, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
```

```{r}
set.seed(1)
cv.out = cv.glmnet(x[trainIdx, ], y[trainIdx], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
```

```{r}
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[-trainIdx, ])
sqrt(mean((ridge.pred - y[-trainIdx])^2))
```


```{r}
lasso.mod = glmnet(x[trainIdx, ], y[trainIdx], alpha = 1, lambda = grid)
plot(lasso.mod)
```


```{r}
set.seed(1)
cv.out = cv.glmnet(x[trainIdx, ], y[trainIdx], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[-trainIdx, ])
sqrt(mean((lasso.pred - y[-trainIdx])^2))
```
- lasso gives a close rsme to the best 5 subset but it is still worse then it. We will now try different models.
```{r}
out = glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef = predict(out, type = "coefficients", s = bestlam)[1:(nFeatures + 1), ]
lasso.coef
lasso.coef[lasso.coef != 0]
xTest = model.matrix(G3 ~ ., testData)[ , -1]
pred = predict(out, s = bestlam, newx = xTest)
sqrt(mean((testData$G3 - pred)^2))
```

-Decision Tree
```{r}
library(rpart)
library(rpart.plot)
tree_model = rpart(G3 ~ ., data = trainData, method = "anova")
print(tree_model)
rpart.plot(tree_model, main = "Decision Tree for Student Performance")
tree_pred = predict(tree_model, newdata = testData)
tree_rmse = sqrt(mean((testData$G3 - tree_pred)^2))
cat("Decision Tree RMSE:", tree_rmse, "\n")
plot(tree_pred, testData$G3, xlab = "Predicted G3", ylab = "Actual G3", main = "Predicted vs Actual Final Grades (Decision Tree)")
printcp(tree_model)
printcp(tree_model)
optimal_cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]
pruned_tree = prune(tree_model, cp = optimal_cp)
rpart.plot(pruned_tree, main = "Pruned Decision Tree for Student Performance")
pruned_pred = predict(pruned_tree, newdata = testData)
pruned_rmse = sqrt(mean((testData$G3 - pruned_pred)^2))
cat("Pruned Decision Tree RMSE:", pruned_rmse, "\n")
```
Lets try using the two the two features that the decision tree used for a linear regression It was worse then the best 
```{r}
twoFeaturelm = lm(G3 ~ absences + G2, data = trainData)

twoFeaturePreds = predict(twoFeaturelm, testData)
sqrt(mean((testData$G3 - twoFeaturePreds)^2))

```
```{r}
#install.packages("randomForest")
library(randomForest)
set.seed(1)
bag.grade = randomForest(G3 ~ ., trainData, mtry = nFeatures, importance = TRUE)
bag.grade
```

```{r}
summary(bag.grade)
bag.preds = predict(bag.grade, testData)
sqrt(mean((testData$G3 - bag.preds)^2))
nFeatures
```

Do CV to find the best number of variables to consider

```{r}
k = 5
set.seed(1)

folds = sample(rep(1:k, length = nTrain))
cv.errors = matrix(NA, k, nFeatures, dimnames = list(NULL, paste(1:nFeatures)))

for (j in 1:k) {
  train_fold = trainData[folds != j, ]
  val_fold = trainData[folds == j, ]
  for (i in 1:nFeatures) {
    cvModel = randomForest(G3 ~ ., data = train_fold, mtry = i, importance = TRUE)
    preds = predict(cvModel, val_fold)
    cv.errors[j, i] = sqrt(mean((val_fold$G3 - preds)^2))
  }
}

mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors

par(mfrow = c(1,1))
plot(1:nFeatures, mean.cv.errors, type = "b", xlab = "mtry", ylab = "CV RMSE", main = "5-Fold CV for Random Forest mtry")
best_mtry = which.min(mean.cv.errors)
best_mtry
best_mtry_model = randomForest(G3 ~ ., data = trainData, mtry = best_mtry, importance = TRUE)
preds = predict(best_mtry_model, testData)
sqrt(mean((testData$G3 - preds)^2))
```
5-fold cv for ntree
```{r}
k = 5
set.seed(1)

num_tree = c(300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500)
folds = sample(rep(1:k, length = nTrain))
cv.errors = matrix(NA, k, length(num_tree), dimnames = list(NULL, paste(num_tree)))

for (j in 1:k) {
  train_fold = trainData[folds != j, ]
  val_fold = trainData[folds == j, ]
  for (i in 1:length(num_tree)) {
    cvModel = randomForest(G3 ~ ., data = train_fold, mtry = best_mtry, ntree = num_tree[i], importance = TRUE)
    preds = predict(cvModel, val_fold)
    cv.errors[j, i] = sqrt(mean((val_fold$G3 - preds)^2))
  }
}

mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
```

```{r}
set.seed(1)
best_mtry = 18
par(mfrow = c(1,1))
plot(num_tree, mean.cv.errors, type = "b", xlab = "ntree", ylab = "CV RMSE", main = "5-Fold CV for Random Forest ntree with mtry = 18")
best_ntree = num_tree[which.min(mean.cv.errors)]
best_ntree
best_mtry_ntree_model = randomForest(G3 ~ ., data = trainData, mtry = best_mtry, ntree = best_ntree, importance = TRUE)
preds = predict(best_mtry_ntree_model, testData)
sqrt(mean((testData$G3 - preds)^2))
```

```{r}
importance(best_mtry_ntree_model)
varImpPlot(best_mtry_ntree_model)
```
try only using the best 4 features that are used G2, G1, absences and failures and the current best ntree
```{r}
set.seed(1)
mostImportFeaturesRandomForeset = c('G2','G1','absences','failures','G3')
mostImportTrain = trainData[, mostImportFeaturesRandomForeset]
mostImportTest = testData[, mostImportFeaturesRandomForeset]
mostImportModel = randomForest(G3 ~ ., mostImportTrain, mtry = ncol(mostImportTrain) - 1, ntrees = best_ntree, importance = TRUE)
preds = predict(mostImportModel, mostImportTest)
sqrt(mean((mostImportTest$G3 - preds)^2))
```
Try k-fold cv for number of trees using this subset when only consider these features for splits
```{r}
k = 5
set.seed(1)

num_tree = c(300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500)
folds = sample(rep(1:k, length = nTrain))
cv.errors = matrix(NA, k, length(num_tree), dimnames = list(NULL, paste(num_tree)))

for (j in 1:k) {
  train_fold = mostImportTrain[folds != j, ]
  val_fold = mostImportTrain[folds == j, ]
  for (i in 1:length(num_tree)) {
    cvModel = randomForest(G3 ~ ., data = train_fold, mtry = 4, ntree = num_tree[i], importance = TRUE)
    preds = predict(cvModel, val_fold)
    cv.errors[j, i] = sqrt(mean((val_fold$G3 - preds)^2))
  }
}

mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors

par(mfrow = c(1,1))
plot(num_tree, mean.cv.errors, type = "b", xlab = "ntree", ylab = "CV RMSE", main = "5-Fold CV for Random Forest ntree with Important Features")
best_ntree = num_tree[which.min(mean.cv.errors)]
best_ntree
best_mtry_ntree_model = randomForest(G3 ~ ., data = mostImportTrain, mtry = 4, ntree = best_ntree, importance = TRUE)
preds = predict(best_mtry_ntree_model, mostImportTest)
sqrt(mean((testData$G3 - preds)^2))
```





